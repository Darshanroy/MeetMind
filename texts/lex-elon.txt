hello everybody Welcome to our rag hack live streams rag hack is a free Global hackathon happening from September 3rd to 16th rag is a technique to get a large language model to answer questions based off your data we have more than 25 live streams teaching you how to build rag apps with different Microsoft Technologies plus 10 cash prizes for the best rag apps in each category so we super excited to have you join us for this rag hack and now a word from our friends at reactor hello everyone and thank you for joining us for todays session my name is es skia and Im one of the event planners for the Redman reactor space before we get started today Ive got a few things to go over so if you could please take a moment and read our code of conduct uh basically we what it says is that were seeking to provide respectful environment for both our audience and presenters we encourage engagement in the chat but please be mindful of your commentary remain professional and on topic uh useful links will be shared throughout the chat so keep an eye out for that and the session is going to be recorded and will be available on demand in 24 to 48 hours on the Microsoft reactor YouTube channel and that brings us to todays session its going to run approximately one hour with time for questions throughout and with that said I will now turn it over to our speaker hello everyone uh so Im Cedric Vidal uh Im a principally Advocate at Microsoft and Im going to talk to you today about a way to improve the uh uh performance of your rag setups with a method called raft uh it stands for a retrial augmented FY um and lets jump right into it um and so first of the the way Im going to organize uh this presentation is uh first Im going to um present the basic concepts uh such as what is rag what is raft and then Im going to uh do um a run through of a repository um that Ive prepared that you can use to apply the method to your projects and work you through um the code and how to use it but first um rag so Im sure you all familiar with rag but real quick um so the principle is that you are you ask a question um to uh the system to rag setup and in order to ground the results um and give a domain specific answer you search for documents in some kind of data store using some kind of vector similarity search you retrieve the documents um that are the closest to the uh um to your question you bring them into the context and then you build a new prompt that you sent to Z llm which contains both the question plus the document the parts of the documents that are relevant in order for the LM to generate a grounded answer so thats basically how rag Works um the the problem with um rag is that sometimes it can be the answers can be frustrating because the documents that are retrieved from the database may be um irrelevant um so because the way the documents are looked up is based on Vector similarity search and you might have a document that um while close to your question uh from a vector embedding standpoint from um an eding standpoint is actually not really relevant to answer the question is that actually adds noise and um drives the attention of the model away from what matters so um different ways to uh improve the situation is to one way to improve the situation is using that method called raft which uses documents from your domain knowledge to find tuna model um um optimize for your use case um but what does fine tuning what is fine tuning what does it mean um so it refers to um customizing a pre-trained model so basically when you think about uh GPT 3.5 turbo GPT 4 llama or those kinds of model those have been pre-trained on a huge quantity of data uh from the internet um that data um so those models are extremely powerful and they are usually very big and they know a lot of things um but when it comes to your own domain when you are an Enterprise or you want to build a product or an app um usually you have your own domain expertise you have for example if you work for LW uh in in the low industry or uh in health care or um in gaming or whatever uh industry specific specifics uh you work in um those very big uh models might have an idea general idea of the industry and general idea of the of the domain but might not have the very specifics that you that you own um so what you want to do is use the data specific to your domain and train the LM to be better at answering questions on that domain there thats where fine tuning comes in the principle is you take a pre-trained llm uh such as GPT 35 uh or small Lama model 8B um and you take a data set and you create you combine the two um and you create a fine-tuned version of the LM that knows about your domain um why would you be doing that um there are multiple reasons uh I mentioned Dom domain I mentioned domain specificity but its not the only version another version is performance uh very big LMS as powerful as they may be um they may be a bit slow or or costly to run um so when you fine tune on a smaller model because its smaller its going to be uh faster to execute um and its going to be cheaper uh the cost to run the model is is going to be lower because the number of parameters is going to be smaller uh its going to consume less memory less gpus uh its going to take less time to execute so usually its going to cost less money um and its gonna the the the last reason why you want you might want to do that is differentiation because being specific to your domain your industry and having been trained on your uh prop ratory uh private information you can create an edge compared to your competitors um and and sometimes that age makes a whole difference and can be uh the difference between users wanting to use one company or using another and because we are the beginning of the AI race we are at a time where um differences between companies are very uh very important and um the faster you go the more you find your model to to to be specific the more your users are going to be in some device to to come to you in use your products um but its very challenging um fing is uh is complicated um first of all you need data um and the the thing is its not just data as any kind of data um because most companies most Enterprise have data they have their documents they have their analy expertise in the form of Word document PDF documents or structured databases but in order to train an llm its not just you take a database or you take a bunch of uh Word documents or PDF documents and you tell the LM hey just learn about it it doesnt work that way um the you need to provide a curated um data set set prepared in a way that is tailored to the task at hand so for example in the in like most rag systems the goal is to be able to ask a question and get an answer so its what we call a QA type of task so you need to prepare a data set to with questions and answers about your domain and thats where usually it becomes difficult because uh you might have a ton of documents but you usually dont have uh it formed as a set of questions and answers second is comput its very hard to combine gpus nowadays uh its very costly its its scars um hard to get the third one is expertise um its a its still a complex subject um coming up with the people that know how to uh train those models how to prepare the data um is uh those people are hard to find until now so um you I mean a few recap of different domain adaptation techniques uh FSL is basically uh um in context learning its basically when you give a system prompt everybody has done it at this point most likely when you have GPT or L and you you give instruction ctions so that that the model is going to follow um then you have rag retrial um augmented generation uh with the limitations that I mentioned is that usually uh some documents are going to be retrieved that actually are noise and not relevant to answer then you have fine tuning um where are we at so all of those approaches I just mentioned have pros and cons um in context is super easy but uh only go so far um fine tuning is very efficient but does not allow I mean its limited in uh you need to fine tune which is very expensive every time you want to add new knowledge and rag comes with u the um irrelevant documents that I mentioned so ucle came up with a method called raft that provides an answer to uh all those previous answers and provides a new approach that combines fing and right together lets see how it works so the principle of raft um is like a student passing an exam um its either closed book or open book when its um Close book it means that its basically fine to name its you you have work for weeks and months or year to prepare for an exam and you arrive the day of the exam and you dont have access to anything no books no internet no nothing you do the exam only from memory only from what you have learned this is fine tuning and you might hallucinate like even as a student uh you might be um you might misremember something you might forget something you might confuse uh something um on the opposite open book is when okay you might have studied you might have prepared the exam of course but when you arrived at the um the exam mostly you have a pile of book in front of you you have all the knowledge but you dont know in which book the information is uh because those books are ended uh to you um at the start of the exam and you had not seen them before so even if you have all the information required in order to answer the questions and pass the exam uh you dont know in which book to look so its going to take some time to go through all the um uh first pages and to find where the information is and when you look at the titles of the sections you might um find red herrings you might um think that one um part of the book is actually relevant you spend time reading it and then at the end its not um so thats right so raft is basically building your own librarian um its basically going to the library uh where the librarian knows about all the books um and you can ask him a question and he knows because he has read all the books he knows which books are relevant to answer the questions and he can direct you to those books and answer your Music questions um real quick an example using uh using vampires uh vampire domain so what I what I did is that I took um um and you know what actually Im going to switch to the demo now and use an actual Yeah a different domain Im going to use surfing for my example but Im going to switch to real code now we dont need the slides anymore um youre GNA have the slides so you can still go look at the slides but Im going to switch to the demo so the raft uh distination recipe um repository that Im showing here actually you have on the very last slide you have a link here a QR code and you have an aka.ms uh link so slash ra- recipe and when you enter that link youre going to be redirected to the git repository that Im going to uh show you now um yeah by the way let me go to the S repository okay raft recipe so so this is the G repository that implements uh that its not doesnt Implement raft is implemented um by ucle so there is another um aka.ms link for raft repo so the this is the raft repository which was created by UC B researchers and the raft decision recipe here packages it in a way that is super easy to use on a AI so it contains um infrastructure as code to provision everything you need to um spin up all the resources on AI to run the notebooks and it contains four notebooks one for each step of the rough process uh the first one is synthetic data generation the second one is fine tuning then deployment of the fanun model and then finally evaluation of the funed model um so I have already uh checked out the repository oh by the way I recommend using Cod spaces there is a link here that you can click which is going to open the project in a GitHub Cod spaces um the Cod space is pre-built so it should be pretty fast to get up and running um let me switch to uh my Cod space here where uh everything is already loaded um um and let me show you the first notebook the synthetic data set generation notebook so um lets I should I mentioned the closed book and open book I mention raft uh heres another uh diagram that uh is interesting so Im going to work you through that uh process so the principle of raft is that it takes documents from your domain uh knowledge so for example a set of Word documents or a set of PDFs um and splits them into small chunks um indexes them and for each of those chunk is going to use a big llm so you can use your multiple LMS right now the repository is set up for Lama uh 3.15 405b but uh soon um we were going to have put for for GPT 4 but for now only Lama 3.1 for 45b so for each of those chunks its going to generate a few things its going to generate multiple questions and for each of those question its gonna generate um an answer and a chain of thoughts chain of thoughts is basically the um the steps the the thread reasoning to that leads to the answer that um The Chain of Thought has its own paper um and thats a paper that came up with that method that um dramatically improves the quality of the answers so raft leverages chain of thoughts in addition to the answer and Chain of Thought for each of those question we are Genera we are including some Oracle chunks and distractor chunks the reason why were doing that is to teach the model to to learn to pay attention to what matters uh by including distractor document distractor chunks so chunks that are actually irrelevant to answer the question uh the llm learns how to ignore those and pay attention to what matters um then after we have generated that big list of um questions answers chain of thoughts with their Oracle and the structure chunks um we are going to split that um uh that data set into three uh splits the training split the validation split and the evaluation splits um before I get into the details of explaining what those are let me run the raft common line um I mean let me go through the notebook here so first of all um the first step is to set up the raft repository because the the raft repository uh is a UC Bley project and they dont have a python library that you can peep install uh sadly yet so um in order to work work around that limitation um the setup underscore raft Dosh script here is going to check out the project locally here um and in a location where the clis are going to be available and the notebook is going to be able to invoke them directly um and then there is so before you can run all of this um so the impress structure is available as code with a tool called a so you can just do a up I mean you first need to do um a o lugin uh which I have done already so Im not going to do it again uh then you can do HD up which is going to provision the infrastructure and same Ive done it already so Im not going to do it again and whats interesting here is that you also have um um infrastructure suit of infrastructure test that verifies and I can run it again here um that verifies that everything uh is running um as expected that all the endpoints are available and can be used by the scripts um and whats happening uh let me run that in the common line to make sure that everything is working yes it is working so maybe its just my notebook whats going on with my notebook uh okay Im going to restart Music the canel let Im going to execute H uh here I dont need to execute that lets Music go huh okay I have a problem with my setup here Im not sure whats going on so so luckily I had executed my notebook before um okay so Im not sure whats going on anyway its okay I can I execute that yes so why it is not running okay Im not sure whats going oh interrupt if I interrupt this okay it seems that I have a technical problem with my notebook uh let me in the back Im going to open um in the web uh version of codespace just in case its a problem with my vs code um okay I dont have okay okay technical difficulties during uh during a live uh its great anyway Im going to skip this uh so as you can see it works on the common line I have a problem with my notebook for whatever reason um so Im going to ignore that and just uh walk through the steps so um here after you have uh verified that um everything in the infrastructure is set up correctly all the end points are deployed correctly um The Notebook is going to show you youre going to select which um data set Im sorry for the scrolling I was looking for where I was so um the repeat comes with a few samples so here in the sample data directory you have four uh domains uh one about surfing one about vampires one about dun the movie uh one about bats um so I used to have to use the vampires uh um data set uh as default but believe it or not uh talking about vampires was yielding some pretty uh uh awkward questions which did not pass the um safety mechanisms of azure so I switched to surfing where uh its less controversial and there is no such risk of violating those um fatures so here we go surfing but you can at home uh try with any one of the other domains or documents uh here here I use a single PDF for sake of speed uh but you can point at full directory you can also upload and is highly encouraged to um add to the repository your own folder with your own PDFs if you have Word documents convert them to PDF uh first um then here um so you specify the the data sets the documents that we uh are going to to load uh to um to analyze um then here were displaying the document so as you can see its a PDF about surfing um and as you can see that PDF does not contain any questions and answers um so to speak its just a document that explains what surfing is U talks about the history the key uh the key people in the sports uh the different types uh you see standard paddling longboarding shortboarding uh you know what a personal wecraft is whatever um so a few different things about the uh the domain of Surfing um then you have the raft um do uh python I mean the raft python script which is the main um code that is going to transform to read those documents of the domain and transform them into a data set of questions and answers um this Tak sometimes this takes some time for just one small document it takes roughly two minutes but for bigger data set for example for the talk I gave ear this year I built it took uh two days um for I believe if I remember correctly something like 10,000 qas um so it can be a pretty lengthy uh process so be prepared for that um the the cost to run this notebook with the default small data sets is uh roughly I mean its less than 5 its very cheap uh for a bigger data set like for the build talk preparation it cost between1 and 130 if I remember correctly so its still pretty reasonable to F your own domain on a pretty sizable data set so yeah s were getting a lot of questions about pricing in the in the chat so I shared that on the repo you do have the cost estimate section and you know people are just trying to reason about like if weve got this many pages you know whats going to be the cost and then also whats the cost for uh for inferencing so I was just trying to log into AI uh Studio to to see what the costs are because those costs vary per depending on which model you f tune right yes yes and also the specificity of uh running there is a difference in the cost structure between running a normal pre-trained model uh such as Lama 3.1 and 8B and running a fine-tuned model because um running a fine tune model you um the a platform does fine tuning using a technique called Laura which is very efficient thats why fine tuning using Asing AI so cheap because 150 or 130 like is to generate a data set find tun model uh is very very cheap uh compared to the millions of dollars required to create your own pre-trained model um so this is very cheap and the way this is achieved is using Lowa for low order ranking algorithm but the thing is to run low prediction is quite tricky and you need to have servers to hold the the lower layers um as well as the uh the base layers and put stitch them together dynamically so there is infrastructure cost related to that which is why when you deploy a Lura F tun model there is a fixed uh sorry there is an hourly price in addition to the price per token so you have two uh two two things um whereas when you deploy a pre-train model you only pay by the token s less um yes does I but Im sorry I dont have the actually I do I think I have a slide here on pricing I may have the answer to that question uh do I yes I do um I have a slide for that so here is the cost structure um so like I said the cost of the data set generation plus f t name cost yeah a plus 50 so 130 each inference is different like I said there is um oh and I forgot I didnt include the uh okay Im forgetting here Im sure there is a cost um Im sure there is a cost per hour for f which is not appearing here and I apologize but uh for the uh price per thousand tokens for output tokens its 0134 and at the time of writing that slide so the price may have changed in the meantime so it would be safer to go look at uh and actually um let me see if I have the price on hand uh can I see the actual price Music here let me bring a window so Music here AI did I have ai Studio open already here yes I do so actually I may be able do I have a f tune model thing is I dont have a new one here so but the price may appear the price for fun may appear even um let me see s l resing inter ter yes thats what I thought okay so here you have the the full price structure so fine tuned fine tuned okay so as you can see and the price has changed actually since I did it go yeah its actually went down thats great its less expensive um so um um for input tokens its 003 per th000 tokens for um output its 0.0000 61 and there is a hosting cost like I mention for Laura which is 74 per hour so its not very expensive is that cost this was another question will that hourly costs just start accur as soon as its up so basically like as soon as your model is youre paying the hourly cost or can you like pause it thats a very good question it automatically poses um so yeah now its great no its a very very very cost efficient solution uh really uh in my opinion severs uh is is is a is a game changer I mean its a very very very coste efficient way to deploy fine tun model and model in general uh so what the way it works is that as soon as you use it the Lura uh the Laura servers are going to stay up for an hour but when you stop using them uh after an hour theyre going to be taken down so its not by the minute its still by the hour but given that given that it only cost .74 so its not the end of the world its its pretty cheap still but after an hour its going to be taken down and its youre not going to pay forever for it so its very very good um like for example if you are like even for development like when you use uh the server only occasionally and you have the code standing still and not using it for some time its perfectly fine thats great yeah I didnt I didnt realize that the hourly cost was like that theres another question theres lots of questions actually can we download the fine tud model for running locally no you cant no so so there is a reason for that uh the its actually so there are some pros and cons obviously so using a platform such as uh aure SS is awesome for the reason we just mentioned uh its super cheap its super easy like Im going to show to you how easy it is like it literally all infrastructure you have nothing to care about uh its everything is done for you um um and its also from um like a license standpoint is also very interesting because when you use a zero AI to fine tun model you can use the fine tune model for commercial purposes can whereas when you uh find you download the weights and you do the fing yourself on your own GPU there are some restrictions depending on the model uh and so for um like Enterprise use cases zi offers better uh licensing generally speaking of course I invite you to look into the specifics be sure you understand uh how everything works but its its um its a good solution forent nice okay so getting back to your to your repo there was a question about the generation step was how does it come up with those questions oh uh excellent question so um let me go back to my diagram oh no actually Im going to use um Im going to use a slide that actually has skipped um no do I have a better okay let me okay let me use sorry Im trying to find the best way to answer that question um Let me show sample data so here is an example of the generated um data set so the way it works is um its GNA take a chunk so here um heres a chunk so sorry its because its an list it only displays the the beginning but uh lets say that uh we are sorry Im finding my words uh lets say that we are using looking at that paragraph its going to take that paragraph look at whats in it and say for example the term surfing refers to a person writing a wave using a board so were going to ask a bigm such as LMA 3.1 45b hey heres a paragraph come up with a list of one two three or four its configurable theres a parameter for that come up with a list of questions about that paragraph so it could be uh what does the term surfing refer to so basically turns the the the knowledge the facts around and and invents questions about those facts and whats very very interesting is that it leverages a property of those large language models for which are very good at which is um imagining things like um um like coming up with with coming up with uh with things where usually uh its a problem like when you dont want um when you query a model you dont want it to give you uh answers that are wrong that do that do not exist but when it comes to generating questions on the contrary its um its its a very good property to be able to generate things um and what whats interesting is that so you use that property to generate the questions and then you use the other property which is instruction following in grounding when it comes to generating the answers so use a different capability of the big model when it so at that step let me go um change again to oh my God Im skipping through the this is very bad sorry for the scrolling I hope its not too dizzy uh so uh after you come came up with a question using that capability to imagine things youre going to ask the mon to generate an answer um which requires to be precise and correct but this time you ground the question with the Oracle Chunk from which the question was generated and you you ask specifically the model follow the instructions like ground your answer into that Oracle chunk and by the way after you can also evaluate and verify that those answers are grounded using a different llm and this is whats included in the evaluation notebook the last notebook and I dont think were gonna have the time to look into I mean we might Im not sure but thats a different topic but uh yes uh thats how the questions are generated and the Chain of Thought also is a very interesting property because by asking the model to generate the answers by using chain of thoughts and asking it to go step by step thats how you get you maximize the chances of so it doesnt eliminate hallucinations but it maximizes the chances that your answer is going to be correct um and if I open Im pretty sure sorry I I should have uh no its not very convenient let me look at the data set here and the generated file so here are the generated files if I take uh the train and Im going to take the first one Im going to create a new document and uh sample Json and Im going to um format it format document yes so it should be easier to explain so um here is the generated question here is the Oracle context uh and can I uh display and go back what WP yes awesome so heres um heres the Oracle context it is uh held in in high estimation very be old after use and okay Im not sure what that is uh when Mark Twain visited haai in 1866 he wrote blah blah blah okay so heres the oracal context and here lets look at the question which was generated what was a national Pastime Mark TW observed in Hawai in 1866 well actually were lucky because thats literally uh what I was looking for in um here um so he wrote I mean if I search for Luke um so I was looking for Luke in the OR context but uh actually I stumbled upon a different thing which is the Chain of Thought that I was mentioning so whats interesting is that the Chain of Thought answer says to answer the question we need to identify the national Pastime Mark twn observed in Haw in 1866 and it goes step by step the context provided gives us the necessary blah blah blah we look for the part of the context that directly mentions Mark Twins visit to Hawai we find it in the sentence Mark twn visited ha he wrote in one place We Came Upon a large company blah blah blah uh and quote from this sentence we understand that Mark Twain observed a specific activity that was considered in National ptime in Hawai in 18 in 1866 the activi is explicitly mentioned surf bathing and the final answer is surf bathing so thats how it works step by step using the oral context it gives you an explanation step by step how it goes about generating the answer and finally it extracts the key piece of information and um and puts it after the answer token and thats what we are going to fine-tuned the the smaller model on um and whats also very interesting is that by fine tuning uh using the chain of because the whole thing here is going to be used in the answer for fine tuning is that it links somehow it links the question to that answer with everything in between with the chain of thought with reasoning so it teaches the model not only what the answer is but why thats the answer so its really teaches your model to become an expert at your domain and of course when we look at such a public data set uh most large language models uh pre-trained on the internet know about surfing I mean lets be honest but the uh this is because for the sake of that open source public repository we need to use public data sets but the principle is that when youre going to use the raft methodology on your own Enterprise proprietary data set youre gonna give it information that is not public and thats how youre gonna train the model to to learn youre going to teach the model about something that is not public that only your company knows sorry that was a very that was no its a great it was a great explanation I love all the questions we do have one more question I know were were taking up a lot of time but these are great questions so when youre doing this generation stage I presume its using our like gbd4 or for its a gbd4 right that its using to generate these answers so right now right now its l 3.1 405b oh okay okay 405b okay so youre using a large model yeah so its a large model so youre using a large model so how do you you avoid the like rate limits like what sort of TPM tokens per minute capacity do you need on that large model when youre doing this massive generation okay so thats an excellent question so to be honest when we were preparing uh the build to earlier this year we run into it we run into issues because uh we we run a pretty massive data set generation job like I said uh we generated a bit more than 10,000 uh data points uh so we did run into TPM limit so first of all the codebase has um by the way we because our generation was so massive uh we literally tra went through all the problems you can imagine with the UC raft code base and we did so many PRS on their code base to make their code way more robust so as part of that work um we I the code now is do automatic retries like catches issues um so its robust like if it hits a TPM limits its going to wait and apply exponential uh backup and retry later on thats one one part of the answer but the other part of the answer is that okay its good the code is robust the code is gonna you know wait and retry when the TPM limit goes is is back I mean when you have TPM Avail again but still um you might face a situation where its annoying like you want to go faster so what you can do is uh there is an amazing open source um project uh that allows to uh load balance um across multiple endpoints uh and the name of the open source project obviously Im blanking right now uh I think I mention it somewhere yes okay Im saved uh I I WR it down so its called light llm and that small piece of software is is amazing um its uh and I was previously working at Microsoft in uh the Microsoft for startups uh team and uh I was working in a partnership with like YC y combinator startup and that startup was part of the last uh batch and they develop an open source um open AI compatible uh proxy that allows you to do load balancing across many kinds of llm so what you can do is install that uh and thats what I used for the build talk I spin up uh I think from memory I spin a spin um uh I think I wrot it down yeah I had one Mass endpoint and two uh map endpoint so to be honest the map endpoint are pretty expensive this is highend uh a or H I forget yeah a 100 uh gpus each of those has like eight eight a100 so this is a very very big setup uh and I had two of those um um thanks to my colleagues in the in the product teams um and so I had done um a load balancing between those map end points and that mass and endpoint Mass stands for model as a service and thats what the notebook uh here here is using um which is very very cheap map um is different you dont pay by the token you pay for the infrastructure you pay uh for even when you dont use and its more expensive uh but its low latency uh so depending on your use case it might be interesting its harder to set up because you need to provision it manage it uh think about uh taking them taking them down when youre done with it but uh in that case um yeah I was using a mix in order to maximize my throughput uh and bying those three I was getting 350 token per second so here is the answer to the question right thank you thats great um how much time do we have more we have 12 minutes left 12 minutes left okay uh so okay awesome uh so um shall I continue or do we have more questions yeah keep going okay so um I forgot where I was but uh so I was going through um we had uh I I had finished showing how the the the generation process is I think or I was going through um yeah oh actually I had done everything in my notebook actually I forgot um so lets look at the sample I had done it so as you can see here so when you run the sorry when you run the the ra. P script here it goes so it takes some time uh at the end uh its going to generate the data set then the notebo is going to work you through uh reformatting the files into a simpler formats that are more suitable that are suitable for like working with simpler and also suitable to uh to upload to the um sess fine tuning uh API and here I was uh showing you the table but um whats more interesting is that here I actually look at a specific Oracle context and the specific question that is generated so as you can see here the question is what island in Hawaii is known for having some of the best waves in the world uh I think its Mai Im not sure oo maybe okay apparently its oo I did not know um so um the um so you have the Chain of Thought but Im going to skip it quickly because I I mentioned it before uh and then you have the instructions that uh is like assembled which is going to be used for uh F tuning which contains the list of documents uh as well as the question uh then we do the splitting into training validation and evaluation like I said but let me explain what those are so the training splits um so by the way this is not specific raft um the splitting a data set between those three splits is standard machine learning the principle is that uh the training uh split is used to update the weights of the model thats how thats how you you teach the model new knowledge uh the validation split is not used to update the weights but its used during training to measure whether we are improving the quality of the model or on the contrary if we are diverging so the valuation split also is very important because thats what Drive the model in the right direction the evaluation split um is kept aside it is not used at all during training it is used on after the final fine-tuned model has been produced to evaluate it so the evalution speed here in the context of that notebook is going to be used only in the last notebook the evaluation notebook um um we export uh the different splits in the proper format so basically thats where that um uh notebook ends once youre done with that notebook you have all the files that have been generated into the data set directory in the Sur in the uh Dash files subdirectory with you have all the formats so uh HF is the internal raft format uh ft is the format um optimized for a your AI F tuning then you have the second uh notebook and I dont think Im going to have the time to go through all of them but uh very quickly um by the way all of this is infrastructure as code it doesnt require you can do all of this through the UI I mean except for raft but fing deploy M can be done through the AI Studio UI but here the goal of that notebook and by the way I believe this is the only notebook that provides you with a complete endtoend um demonstration of how to do uh data gen fine tuning deployment and evaluation fully as code code first um so here it shows you how to find um to start a funing job um and okay Im going to schim through it very quickly uh at the end what matters is what at the very very very end here you submit the funing job and you can when you click on the link here uh I opened the link already and I opened it here so whats very interesting here and oh the funing job completed I started it at the beginning of the talk um and you can see a visual representation inside ml Studio you dont have to look at it if you dont want to but I like to look at it because I like like to see whats going on under the hood so when you click on the link you have the possibility to navigate different components of the fing Pipeline and look at every uh every block uh and see whats how its working um and after its finished the model is going to be available in the deployment section in AI studio and ml Studio as well uh and sorry not in the deployment but in the uh fine tuning uh section here yes so here we have our fine-tuned uh model which is a result of the fine tuning job um and so after youre done with fine tuning you can go to the deployment notebook and same cut first everything is automated you just go through the sales um and its going to um deploy and actually you know what I can uh I mean if my um if my uh notebook issue is resolved which it looks like it is yes so um Im going to go through those um so here I am waiting for huh uh okay I have a problem here because it doesnt find the model which uh was fine tune when it should a fine tune model name why isnt it D1 D3 oh looks like I have a bug okay uh huh okay I have a bug it seems uh that I will need to fix um so Im going to interrupt Im going to restart and Im going to uh run all to go faster and hopefully this time it finds the no it does not um D3 CB why isnt it finding my model that is strange okay Im going to have to troubleshoot that this is not expected um anyway anyway when its working um its um its going to and you know what Im going to try to deploy to run the model wait for model yeah no okay its not going to work okay Im going to have to fix that but um the um so that notbook is going to deploy uh that model what you what I can do is do it manually um so Music here um um so this this does manually what the notebook does programmatically so once this is done youre going to have an endpoint a sever less endpoint of your fine tun model that you are going to be able to consume um as any other model and so what whats I mean the key take away here is that uh I mean once I have fix that bug uh that uh recipe here is really an endtoend solution that allows you to super easily um f your own model using your own document you have nothing to do like you just provide the documents which usually you have um and it generates the data set automatically it does a fine tuning automatically it even evaluates the quality of the of the model all you have to do is provide the document and be patient and at the end you have your own model so Cedric we had a question a couple questions can you you fine-tune a fine-tuned model is that even a thing so uh that is a good question um so not using um I mean in theory yes um in practice not so not so much I mean because when you think about it a pre-trained model is a model that has been fine- tuned already because a pre-train like a model that such as Lama 3.18 b or GPT 35 turbo those models have not been created in one go like first you have pre-training then you have a like two or three I forget uh follow up steps to fine- tune the models to apply rhf um reinforcement learning using human feedback or other kinds of um um alignment uh techniques um those are fineing techniques approaches what like variations um so when we do fineing um like when we when we talk about fine tuning here its about F tuning a pre-rain model using your own data but yeah in theory you could F tuning again just um on as your AI its not going to be uh practically possible because fing is about creating a lower layer and it doesnt make much sense to update the lower layer a game um so yeah okay and then another question so you know we have that generation step once weve done that generation set we can in theory use that generated data for multiple fine-tuning models right like if we wanted to try fine tuning a 7B uh you know 70b Etc yeah so um the the model comes um so first over here at the top when on in the fing notebook you can select which that you want to find one so if you go towards the Top If you search for parameter yes uh here you have the possibility to modify the base model that you want to find tune and if you look at the uh parameters directory here you have three predefined configurations that you can look at um the 3.18 B is a different one and you have values for each parameters um and you can update manually the value here uh just make sure that its consistent with whats in the file here because those values have been tested and they are known to work uh all the files here are known to work uh also what you can do is there is um a common line uh script here run allsh uh which is um which allows you to um pass uh yes here uh yes here uh it allows you to so it uses a framework called Papermill which allows to do parameter uh value replacement so basically you point paper at uh one of those um parameter ml files and its going to variab variabe the run and uh use the values inside so basically its GNA be able youre going to be able to run all the notebooks and and switch from uh AG uh .1 8B to 38b or or um a different version and we are currently working on adding support for more models so uh in the upcoming days and weeks youre going to have uh jpt 3.5 youre gonna have F Microsoft 53 those kinds of M are going to appear uh in that uh in that folder uh and um the syntax to um uh to select uh which parameter file is- p so you say run all-b and you point at the yl file and also you are welcome to to submit PRS on the repository if you uh try yourself a different combination which works feel free to uh to submit a PR sweet all right so we are at time I dont know if you have to jump to something now because there are of course more questions yeah no I still uh just uh I think I have let me double check real quick I think I dont have anything no Im uh Im Im okay if there are more questions I can take them okay so we do have more questions about pricing I remember I told you thered be questions about pricing because I think this is a big you know question that people have like how affordable is this going to be so uh one question is like you know as this is happening does your does your billing update in real time or do you only see how much it costs like the next day and could you like set limits to be like I dont want to spend more than you know 10 on this so um huh that that is a good question to be honest so uh the first question is it updated in real time Im not sure and I dont think so I dont I dont know Im not sure yeah but I dont think it does uh I think there is some kind of delay um but um yeah something something to double check regarding the second question um I believe its possible to put like a a limit but its more like its not specific to fine tuning its not specific to seevers its more generic as your question on subscription like when you you put your credit card um on the subscription Im assuming it should be possible to put a limit but uh same I I you can definitely do alerts because I accidentally deployed way too many things on my personal account instead of my work account one day and it it was a very bad day and uh I uh I said uh you know an alert uh but I yeah I dont I dont actually know about limits so you know those are great questions because we you know obviously dont want people to spend more money than necessary uh so another question related to that is does AI Studio have a tool to estimate costs for deployed models uh you did show the pricing in AI studio uh but I dont know if if you know if people are looking for even more detailed cost estimator than that um no I dont believe there is but uh you know I I I believe its rather let me see because when you select the model oh what did I do okay when you select the model you have the pricing here its a simple multiplication and addition so um its yeah it would be better if we had a tool but we dont uh you you have to do the calculation on your own um also be uh aware of the fact that it doesnt take into account like networking you know other costs it the cost displayed here are only for like this one model like the cost of infrastructure of other infrastructure pieces that you might have in your rack set up for example youre going to most likely need a database either if its a psql or another one youre gonna have some additional cost but thats to be expected so its a hard question like how much is gonna cost uh oh but dont we have like a generic like azero cost calculator yeah we do have the azir cost estimation calculator I havent tried using it forun specifically it definitely has you know the estimation for um when youre using AZ open Ai and you put in like your number of tokens so you can look at the Azure pricing calculator to see if it would help with that but I dont honestly know if it has all the information from these serverless models yet I cant remember looking that up uh so those are good things that we can dig into more couple questions on use cases uh like so somebody is working with documents that have lots of internal jargons and acronyms would it make sense to use fine tuning oh yes for that or you know would you do it somewhere something else like providing thats precisely what font excels that okay because like Jon Jon and and like very awkward domain specific terms are precisely what does not exist in public data sets so thats precisely what large language models are going to be bad at so for example if uh you ask a public model um to give you an answer using some concepts for which in your industry in your expertise there is a more like a different word a different term or an acronym uh the the lot the public model is GNA lack specificity and is not going to use the words of your domain whereas h fine tune model you will teach the model what those Jon and acronyms are and its going to use those in the the answers the only catch with that is if the jargon and ACM are so awkward that they that there is no edding Vector to represent them like for example imagine that you have some Jon which mixes like symbols like I dont know like a dll with a weird man I mean I dont know like you see what I mean like asky characters that come in a combination that is so rare that the unbending model does not have a vector for that theoretically it might happen but it is such an ed case that I dont think werent you thinking of fine-tuning and bedding models because if you fine-tuning and bedding model then yes so you can do that uh but funing in a model is different subjects uh its more technical than fine tuning an llm for the simple reason that there is no like uh serverless infrastructure as a service to find to n model when it comes to funing an uning model it comes to spinning up your own GPU I mean even if its on the cloud and you use like you rent the gpus but still you need to like spin up the GPU enabl VMS you need to get qua I mean you need to get uh to get your your hands on those then you need to set up the whole python F tuning infrastructure with spy toor or something else uh so its its level 400 like its its a higher level of expertise its possible though okay great and then one other use case um someone was asking about fine-tuning an LM to Output Json uh and of course I mentioned that we do have support for structured outputs in the Azure open AI but those uh arent compatible with the zero data retention policy and whereas if you find tune you know you can um control your data retention so this is a good question a very good question so this is I this is a hard problem because um you you you know how you have usually for most models you have a code specific version like usually you have Lama code or whatever um thats because those models are fine-tuned on huge quantity of code which includes Json documents and when it like J and Json in my opinion okay good openi decided to do that uh it makes developers work way easier but I dont think if it was the most Sound Decision to make in the context of llm Technology because generating Json is actually incredibly difficult for an llm the way its designed like lets lets not lets not forget that an llm is um statistical distribution that for a sequence of tokens generates the most probable next token so when it comes to uh just document where it is a a symmetric structure where each opening um what is the word like um the curly brace each opening curly brace needs to be matched with the closing curly brace this is insanely difficult from a statistical standpoint so it requires a gigantic amount of information to get the model to to generate valid Json and and interestingly enough when you look at the very I mean the first like at the original chpt um it was bad at that like sometimes it would generate J and sometimes it would not sometimes it would be invalid like it would it was bad its only recently that LMS have gotten to the point where they became pretty good at generating uh um structured uh output such as Jon or XML uh but this is very hard so I would not recommend trying to do that on your own or for your own data set I would rather encourage you to choose a simpler type of structure documents structure such as CSV or or tabular tab delimited or I dont know any kind of nonsymmetric structured data format its way easier right and then also I mean and Json is a cursive hierarchical structure right so thats that yeah you you are right I mean yes not only is it symmetrical but its hak also uh so of course yeah I mean CSV not everything can be represented uh efficiently uh as soon as you have recursivity as soon as you have like nested things you need some kind of symmetry but still uh I I would encourage to go with a a simpler um format than uh that yeah thats really really interesting okay one more question is about chunk size and chunk overlap and this is a question we always get when were talking about rag right and I pointed at the AI search team they have a great blog post where they looked at optimal chunk size and overlap for AI search but Im curious like what you know what is what does Raph do like how do they decide on the chunk size and overlap and do they have any tools to help you okay so again very interesting question um thank you for for asking that um so raft comes by default with something called a semantic uh chunk strategy um which it comes from um the um they use the open source Lama forgot if this L index or no longchain sorry they they uses one of the experimental components of longchain um which comes from a paper where instead of chunking fixed size like fixed number of tokens they use some kind of seening approach to make sure that when you cut you dont cut in the middle of a sentence like you try to cut when there is a DOT like at the beginning of a paragraph at the end of a paragraph So some kind of flexibility uh to avoid like losing interesting context um that being said uh even then its its not perfect like sometimes you might be lacking some piece of context which is was just before or just after and so when you create the edding uh for the chunk um it its actually irrelevant um um and um or you get the same Vector for two different chunks which are actually talking about two different things because for example if you take a chunk about vampires and another chunk about bats um sorry first um yeah it it might because some kind of bats are vampire bats you might end up with like a similar Vector when in fact youre talking about two different things or vice versa I mean so you can have false positive or Force negatives um and so its not perfect which is why overlap uh is interesting but I must admit that right now for raft there is no overlap the the the chunks uh so it uses semon chunking but it does not use overlap and its not currently configurable uh would it improve the situation um to be honest I dont know I dont know if its relevant in the context of raft its definitely relevant in the context of rag because for rag you want to uh to to to contain a bit more to avoid Force positives and force negatives but in the context of draft since its about generating questions and answers and Chain of Thought about a specific paragraph I dont see how uh overlap would improve things I might be wrong but thats my like without having think thought too much about it that would be my answer I think its uh it wouldnt help all right yeah thats thats really interesting okay so another question is is it beneficial to use fine-tuning if your documents are in a non-english language is would that be a good use case uh yes of course um by the way before I answer that question just to to be sure I get one thing straight uh and and Im not misunderstood with what I said before so overlap would not help with the raft synthetic data set generation process but it would help with the rags indexing step just to be PR c um so back to your uh and here I am I forgot the question languages other langu English yeah uh other languages so yes yes I believe it will work um but it would um it would work exactly the same way as it would for English what I mean by that is that um there is no like I believe additional benefit or or less benefits uh for other the language Maybe other than the fact that maybe um large language models during pre-training are there is a bias towards English language um knowledge uh parts of the internet so by fening and playing raft on different languages yes you might be able to fix the bias which is naturally occurring during pre-training of those large language models so that would be part of the answer uh it is not back what Im saying here is based on intution it is not backed by data so um caveat here uh but yes I believe it would help a bit um that being said be aware that that would not change the fact that um edding models are better for Lang English language so even if um I believe there is work done and I believe the latest llama 3.1 uh on beddings is better or is it a different vendor sorry I might be uh mixing up with the new open AI text embedding three models are better for international about that yeah we talked about that at the Monday session because Anthony has done some research into that okay so those latests are better but still you are like rough is not going to help is still going to be bound by the accuracy by how good the underlying edding model that you use for the rag setup is yeah and we do have I think there are some embedding models coming out that are specific to a language too then that might be good to use for a vector search um I think theres ones like maybe specifically for Japanese or Korean that would make sense to use because you you still want to get good results during your actual rag call right of course raft by the way um and I didnt mention that earlier but so when it comes to like maching in general when and when it comes to rag specifically uh you have two metrics that matter precision and recall Precision is when um um like good Precision I mean its an indication of how good your retrial product process is at returning documents that are relevant to answer the question um so thats Precision um and oh am I saying it uh Jee Im confusing myself um yes no sorry okay so Precision is an indication its an indication of um how like if you have if you are returning documents that are irrelevant to answer equ so um if you are returning too many documents and some of them are irrelevant thats bad Precision uh recall on the contrary is an an indication of how many of the good documents how many of the documents which you want to to retrieve are actually retrieved by the retrieval process so what you want is high precision and high recall but um yeah those are two different indicators um and raft improves Precision why because um recall is bound by the edding model and um because raft fine tunes the llm that is going to generate the final answer it has no bearings on the Ral process the only thing you can do is once it looks at the documents that were retrieved using the uning model and the um Vector similarity search it can eliminate the documents that are irrelevant so it improves Precision but in order to improve recall you would need to improve the ritual process and fine tune the eding model sorry for the initial confusion oh I always get them confused too always have to go to that Wikipedia article that has like theis call graph AA that do so I I know were going so long but I that does lead me to the question I was thinking is that you know when were using like a your AI search a your AI search has this semantic ranker which does this really nice like reranking of the results it comes back and even gives them like a nice reer ranker score threshold and then you can even like remove the things that dont fit a threshold so you could like in theory if that works you could have results that dont include distractors so you know Im curious if you like if our retrieval is like perfect do we still need raft yes because uh raft optimizes two things thank you for that question uh its actually a subtlety thats Advanced understanding of raft so um raft um like eliminates irrelevant documents thats one way it optimize Precision but also because it learns using Chain of Thought to get straight to the point remember how was showing that it was giving it was like returning exactly the the two words or one word which is like straight the answer um so it it um it not only does it train the model to ignore irrelevant documents but it also trains the model to inside a given document to which paragraph and which sentences and which words of the documents are actually the most Rel to answer the question so yeah its its uh however good whatever you do during retal raft is gonna improve uh the Precision of your setup and um lets not forget that uh so fusion and semantic ranking that you mentioned um like it it does improve the setup but it does not improve uh the semantic uh the initial Vector similarity search its it comes after its its between the its after Vector similarity search and its before context augmented generation so it takes um the list of results and uh which contains relevant and irrelevant documents um and puts the relevant documents at the top so yes it improves Precision but it does not improve a c because if some documents would actually be relevant you want them but they are not retrieved because from a vector similarity search standpoint they are too far away from theing of your question they are not going to be inside the list that AI search is going to semantically semantically rerank so while I highly recommend to use AI search reranking because it does improve um um the efficiency of your setup it is you know each one of those optimizations improve the situation but um there is no single Silver Bullet like you need you need to accumulate the optimization of the Improvement every step of the way to improve your rack setup as a whole that does lead to the final question and we well well call it after this but how do you then choose the best VOR Vector database how do you evaluate that for your you know your rag set H well for thats a very good thats a tough question um like you have many dimensions to that answer uh you know there is the cost there is uh the performance the scalability uh depending on what is the volume like how much data are we talking about then there is the quality like what state-of-the-art um AI features does it have like does it have reranking like AI search has um does it have like uh you know chunk over I forget the name like but um overlap I mean you know each one of those features um so we are like I said earlier we are at the beginning of the AI uh you know um stage and we are still at the stage where you know each one of those vendors I mean their state-ofthe-art and they are pretty much all trying to implement the same state-of-the-art optimizations that come from the papers that are publicly published and available but still still they are still rushing to get those features in and so we still at a point where its not like Wily available so and evenly available so I would say that two ways either you rely on um third party expertise like third party comparisons or you have the expertise and the will to take the time to look at every single one of them and see exactly what are the optimiz optimizations that they that they Implement um what I know is that uh yeah a search is interesting from the standpoint of uh those you know uh big AI optimizations um that are where you get a lot forther back um the has ranking and uh hybrid search yeah hybrid search by the way too is uh another way to uh improve recall this time uh uh hybrid CH does improve recall uh because um in addition to doing Vector similarity search which is based purely on the quality of the embedding model it adds um token based um uh retrieval which is the old style like before AI when we were uh when we had only full text uh search uh databases uh such as Lucen elastic search you know that kind of that kind of stuff the those at the time were using only token based um indexing and search and those whats interesting with toen based search is that they will return um complementary results um where um un bidding Vector SAR search is going to return both a document talking about vampire bats and bats because in the uming space those two set are going to talk about the same thing or if you talk about Dracula like Dracula and vampire are going to be closed in the eding space uh whereas with a token based approach if you were searching for vampires it would not return documents contain toing only about Dracula it would not but if you search for like idas like technical terms uh like for example reference like if you have a catalog of products with or like car parts where you have like uh you know sequences of characters that make no sense to a human like x y z to 3B YZ whatever uh that sequence of characters makes no sense from a language standpoint so in the uning space you will not have a good Vector for that but from a token standpoint using old style um full Tex search if you SE for that keyword its going to return all the documents that contain them and so its going to improve recall um for those situations where you want to look for um uh that kind of technical terms all right this was great uh we we should wrap up now is there anything else you wanted to share for in terms of Link uh well just uh I mean the the link to maybe uh in case anybody uh missed it the link to the um to the uh to the rep to the repository maybe also a blog post that I wrote where I go over um those um uh Concepts so the link is uh recipe no raft uh recipe uh blog datag gen this is a link uh that I can put um in the chat Music um so an article where I go over um some of the concepts that I have presented today um and uh explanations and there is an old BL older blog post raft Das blog uh which was more high level but which is also uh which might also be interesting uh and yeah I encourage to uh go to the brid repository clone it and if you find isues please feel free to uh submit bugs uh I will try to do my best to fix them and I already have one to fix so sweet and well also put all these links in the GitHub discussion thread for this section session which is on the a.m rag Haack and well post that GitHub discussion thread link also in the chat uh so that if you do have any followup questions because there are so many theres so many questions I dont even know if we made it all of them so if you are watching this now or youre watching this later and you do have follow questions you can go to that discussion thread on the rag hack forum and ask additional questions and we will pick Cedrics brain some more sweet all right thank you so much Cedric that was super interesting yeah uh and thank you for inviting me it was a pleasure all right thank you both uh Pamela and Cedric for an excellent session uh we really appreciate appreciate it and thank you all for joining today um we would love to hear your feedback uh so please take a look at the survey link uh the event code for todays session is 23342 and we appreciate you we will see you on the next episode he everyone hope you enjoyed todays session if you arent registered yet for rag Haack uh you can register now at aka.ms rag Haack register and you can introduce yourself on our Forum you can start hacking either by looking at one of our code samples or just starting from scratch if you have any questions along the way feel free to post them in our GitHub discussion forum and you can also attend live office hours um in our Discord once your project is ready make sure to submit it before September 16th at 1159 PM PT to be eligible for our cash prizes and of course make sure to join our upcoming live stream to learn more about rag thanks for joining