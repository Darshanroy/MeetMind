{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task: MeetMind\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Requirements:","metadata":{}},{"cell_type":"code","source":"# ! pip install -q langchain langchain_chroma langchain_community langchain_core langchain_huggingface transformers\n# ! pip install -qU langchain_huggingface\n# ! pip install pytube\n# ! pip install --upgrade --quiet  youtube-transcript-api\n\n# !pip install -qU langchain_pinecone\n# !pip install -qU langchain_groq","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:24.001763Z","iopub.execute_input":"2024-10-21T07:30:24.002316Z","iopub.status.idle":"2024-10-21T07:30:37.381264Z","shell.execute_reply.started":"2024-10-21T07:30:24.002232Z","shell.execute_reply":"2024-10-21T07:30:37.379684Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2. Loading Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport time\nfrom dotenv import load_dotenv\nfrom langchain.chains import create_history_aware_retriever, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_chroma import Chroma\nfrom langchain_community.llms import HuggingFaceHub\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_community.document_loaders import YoutubeLoader\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\nfrom langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.chains import HypotheticalDocumentEmbedder\nfrom dataclasses import dataclass\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:08:30.400785Z","iopub.execute_input":"2024-10-21T10:08:30.401207Z","iopub.status.idle":"2024-10-21T10:08:30.442547Z","shell.execute_reply.started":"2024-10-21T10:08:30.401168Z","shell.execute_reply":"2024-10-21T10:08:30.440913Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_history_aware_retriever, create_retrieval_chain\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"],"ename":"ModuleNotFoundError","evalue":"No module named 'langchain'","output_type":"error"}]},{"cell_type":"markdown","source":"## 3. Links","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Credentials:\n    token: str = \"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\"\n    yt_link: str = \"https://www.youtube.com/live/UrfmlqHgZLA?si=gGsUOyKGz2zmltr9\"\n    groq_api: str = \"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\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:08:38.959049Z","iopub.execute_input":"2024-10-21T10:08:38.960021Z","iopub.status.idle":"2024-10-21T10:08:38.965890Z","shell.execute_reply.started":"2024-10-21T10:08:38.959977Z","shell.execute_reply":"2024-10-21T10:08:38.964798Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 4. Laoding text from video","metadata":{}},{"cell_type":"code","source":"def get_word_count_from_youtube_url(yt_link):\n    \"\"\"Calculates the word count from a YouTube video link and returns the documents.\n\n    Args:\n        yt_link (str): The URL of the YouTube video.\n\n    Returns:\n        tuple: A tuple containing the number of words and the documents.\n    \"\"\"\n\n    try:\n        loader = YoutubeLoader.from_youtube_url(yt_link)\n        docs = loader.load()\n        word_count = len(docs[0].page_content.split())\n        return word_count, docs\n    except Exception as e:\n        print(f\"Error loading YouTube video: {e}\")\n        return 0, None\n    \n    \nword_count, docs = get_word_count_from_youtube_url(Credentials.yt_link)\nprint(f\"Number of words: {word_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:39.331816Z","iopub.execute_input":"2024-10-21T07:30:39.332188Z","iopub.status.idle":"2024-10-21T07:30:39.984094Z","shell.execute_reply.started":"2024-10-21T07:30:39.332140Z","shell.execute_reply":"2024-10-21T07:30:39.982880Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"number of words: 13356\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning the text using regular expressions\ndef clean_text(text):\n  \"\"\"Cleans the text by removing unwanted characters and symbols.\n\n  Args:\n    text: The input text to be cleaned.\n\n  Returns:\n    The cleaned text.\n  \"\"\"\n\n  # Remove punctuation and special characters except for a few symbols\n  allowed_symbols = r\"[^a-zA-Z0-9\\s\\.\\,\\?\\!\\-\\(\\)]\"\n  text = re.sub(allowed_symbols, '', text)\n\n  # Remove extra spaces\n  text = re.sub(r'\\s+', ' ', text)\n\n  return text\n\ncontent = clean_text((docs[0].page_content))","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:39.985718Z","iopub.execute_input":"2024-10-21T07:30:39.986113Z","iopub.status.idle":"2024-10-21T07:30:39.999985Z","shell.execute_reply.started":"2024-10-21T07:30:39.986072Z","shell.execute_reply":"2024-10-21T07:30:39.998515Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# saving the loaded docs to the txt files\n\nwith open(\"texts\\lex-elon.txt\", \"w\") as obj:\n    obj.write(content)\n    \nprint(len(content.split())) ","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:40.002196Z","iopub.execute_input":"2024-10-21T07:30:40.002747Z","iopub.status.idle":"2024-10-21T07:30:40.016057Z","shell.execute_reply.started":"2024-10-21T07:30:40.002696Z","shell.execute_reply":"2024-10-21T07:30:40.014575Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"13356\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. for Chunking and converting into embbedings","metadata":{}},{"cell_type":"code","source":"EmbedModel = HuggingFaceEndpointEmbeddings(model=\"intfloat/multilingual-e5-large\", huggingfacehub_api_token=token)\n\n\n# from langchain_pinecone import PineconeEmbeddings\n# EmbedModel = PineconeEmbeddings(model=\"multilingual-e5-large\",pinecone_api_key='f55bd135-3e72-41d7-8665-55105bbf2e08')","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:40.017804Z","iopub.execute_input":"2024-10-21T07:30:40.018452Z","iopub.status.idle":"2024-10-21T07:30:40.240674Z","shell.execute_reply.started":"2024-10-21T07:30:40.018382Z","shell.execute_reply":"2024-10-21T07:30:40.239371Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter()\nchunck_list = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:40.242511Z","iopub.execute_input":"2024-10-21T07:30:40.243037Z","iopub.status.idle":"2024-10-21T07:30:40.314580Z","shell.execute_reply.started":"2024-10-21T07:30:40.242976Z","shell.execute_reply":"2024-10-21T07:30:40.312425Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 6. Loading chunked docs to vector DB","metadata":{}},{"cell_type":"code","source":"len(chunck_list)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:40.319985Z","iopub.execute_input":"2024-10-21T07:30:40.320451Z","iopub.status.idle":"2024-10-21T07:30:40.329633Z","shell.execute_reply.started":"2024-10-21T07:30:40.320407Z","shell.execute_reply":"2024-10-21T07:30:40.328396Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"18"},"metadata":{}}]},{"cell_type":"code","source":"vector_store = Chroma.from_documents(chunck_list, EmbedModel, persist_directory=\"./chroma_db\")\n# vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=EmbedModel)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:30:40.331135Z","iopub.execute_input":"2024-10-21T07:30:40.331564Z","iopub.status.idle":"2024-10-21T07:31:16.624545Z","shell.execute_reply.started":"2024-10-21T07:30:40.331517Z","shell.execute_reply":"2024-10-21T07:31:16.623032Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 7. Load the LLM","metadata":{}},{"cell_type":"code","source":"from langchain_groq import ChatGroq\nimport  os\n\nllm=ChatGroq(groq_api_key=groq_api,\n             model_name=\"Llama3-8b-8192\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.626335Z","iopub.execute_input":"2024-10-21T07:31:16.626790Z","iopub.status.idle":"2024-10-21T07:31:16.762799Z","shell.execute_reply.started":"2024-10-21T07:31:16.626750Z","shell.execute_reply":"2024-10-21T07:31:16.761357Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# llm = HuggingFaceEndpoint(\n#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n#     task=\"text-generation\",\n#     max_new_tokens=3000,\n#     do_sample=False,\n#     huggingfacehub_api_token=token\n    \n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.764549Z","iopub.execute_input":"2024-10-21T07:31:16.765006Z","iopub.status.idle":"2024-10-21T07:31:16.771365Z","shell.execute_reply.started":"2024-10-21T07:31:16.764958Z","shell.execute_reply":"2024-10-21T07:31:16.769907Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 8. creating all the prompt templates","metadata":{}},{"cell_type":"code","source":"# Define the contextualization prompt for reformulating questions based on chat history\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"\"\"Given a chat history and the latest user question \nwhich might reference context in the chat history, formulate a standalone question \nwhich can be understood without the chat history. Do NOT answer the question, \njust reformulate it if needed and otherwise return it as is.\"\"\"),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.773613Z","iopub.execute_input":"2024-10-21T07:31:16.774271Z","iopub.status.idle":"2024-10-21T07:31:16.786859Z","shell.execute_reply.started":"2024-10-21T07:31:16.774201Z","shell.execute_reply":"2024-10-21T07:31:16.785702Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Statefully manage chat history\nchat_history_store = {}\n\ndef get_chat_session_history(session_id: str) -> BaseChatMessageHistory:\n    \"\"\"Fetches the chat history for the given session.\"\"\"\n    if session_id not in chat_history_store:\n        chat_history_store[session_id] = ChatMessageHistory()\n    return chat_history_store[session_id]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.788486Z","iopub.execute_input":"2024-10-21T07:31:16.788968Z","iopub.status.idle":"2024-10-21T07:31:16.803908Z","shell.execute_reply.started":"2024-10-21T07:31:16.788913Z","shell.execute_reply":"2024-10-21T07:31:16.802627Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define the chat prompt template for QA\nqa_prompt_template = ChatPromptTemplate.from_template(\"\"\"\nAnswer the following question based only on the provided context. \nThink step by step before providing a detailed answer. \nI will tip you $1000 if the user finds the answer helpful.\n<context>\n{context}\n</context>\nQuestion: {input}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.805604Z","iopub.execute_input":"2024-10-21T07:31:16.806095Z","iopub.status.idle":"2024-10-21T07:31:16.816919Z","shell.execute_reply.started":"2024-10-21T07:31:16.806036Z","shell.execute_reply":"2024-10-21T07:31:16.815636Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## 9. Creating chains","metadata":{}},{"cell_type":"code","source":"# Create the question answer chain\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_prompt_template)\n\n# Create the history-aware retriever\nhistory_aware_retriever = create_history_aware_retriever(\n    llm, vector_store.as_retriever(), contextualize_q_prompt\n)\n\n# Create the retrieval chain\nretrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n\n# Create the conversational RAG chain with chat history management\nconversational_rag_chain = RunnableWithMessageHistory(\n    retrieval_chain,\n    get_chat_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.818543Z","iopub.execute_input":"2024-10-21T07:31:16.818959Z","iopub.status.idle":"2024-10-21T07:31:16.838947Z","shell.execute_reply.started":"2024-10-21T07:31:16.818917Z","shell.execute_reply":"2024-10-21T07:31:16.837435Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"user_question = \"Highlight the important points in the podcast!\"\nsession_id = \"u1\"","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.840738Z","iopub.execute_input":"2024-10-21T07:31:16.841451Z","iopub.status.idle":"2024-10-21T07:31:16.854115Z","shell.execute_reply.started":"2024-10-21T07:31:16.841393Z","shell.execute_reply":"2024-10-21T07:31:16.852702Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 10. Inference ","metadata":{}},{"cell_type":"code","source":"# user_questio = input(\"Enter: \")\nresponse = conversational_rag_chain.invoke(\n    {\"input\": user_question},\n    config={\"configurable\": {\"session_id\": session_id}},\n)\nprint(response['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:31:16.856109Z","iopub.execute_input":"2024-10-21T07:31:16.856615Z","iopub.status.idle":"2024-10-21T07:31:18.020031Z","shell.execute_reply.started":"2024-10-21T07:31:16.856566Z","shell.execute_reply":"2024-10-21T07:31:18.018517Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Based on the provided context, here are the important points:\n\n1. **Full Tex search**: The speaker mentions that using old-style full Tex search to find technical terms can improve recall.\n2. **Repository links**: The speaker shares links to the repository, a blog post, and an older blog post, and encourages the audience to clone the repository, submit bugs, and ask questions on the GitHub discussion thread.\n3. **Data generation and fine-tuning**: The speaker discusses the process of data generation and fine-tuning, including the generation of a dataset, reformating files, and uploading to the fine-tuning API.\n4. **Splitting data**: The speaker explains the importance of splitting data into training, validation, and evaluation sets in machine learning.\n5. **Fining deployment and evaluation**: The speaker provides an end-to-end demonstration of how to do data generation, fine-tuning, deployment, and evaluation fully as code.\n\nThese points highlight the main topics discussed in the podcast.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}